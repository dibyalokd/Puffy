â€œâ€â€
XPath Scraper - Pure Scraping Mode with Deep Nesting Detection
Just scrapes EVERY XPath from a webpage, no comparison or healing
Enhanced to detect deeply nested elements and JavaScript-rendered content
â€œâ€â€

import time
from typing import List, Dict, Any
from lxml import html as lxml_html
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options

# Import your database functions

try:
from gptXpathDbCoder import add_xpath
except ImportError:
print(â€[WARNING] gptXpathDbCoder not found, XPaths wonâ€™t be saved to databaseâ€)
def add_xpath(xpath):
pass

# ============================================================================

# CONFIGURATION

# ============================================================================

class Config:
â€œâ€â€œConfigurationâ€â€â€
SELENIUM_HEADLESS = False
PAGE_LOAD_WAIT = 5  # Increased for JS rendering
MAX_DISPLAY_XPATHS = 1000
EDGE_DRIVER_PATH = â€œmsedgedriver.exeâ€

```
# Scroll page to trigger lazy loading
SCROLL_TO_LOAD = True

# Check for deeply nested elements
DEBUG_DEEP_NESTING = True
```

# ============================================================================

# SELENIUM DRIVER

# ============================================================================

def create_driver():
â€œâ€â€œCreate Edge WebDriverâ€â€â€
print(â€[DRIVER] Setting up Edge WebDriverâ€¦â€)

```
options = Options()
if Config.SELENIUM_HEADLESS:
    options.add_argument('--headless')

options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--window-size=1920,1080')

try:
    print(f"[DRIVER] Using msedgedriver.exe from: {Config.EDGE_DRIVER_PATH}")
    service = Service(executable_path=Config.EDGE_DRIVER_PATH)
    driver = webdriver.Edge(service=service, options=options)
    print("[DRIVER] âœ“ WebDriver ready")
    return driver
except Exception as e:
    print(f"[DRIVER] âœ— Failed: {e}")
    print(f"[DRIVER] Please ensure:")
    print(f"         1. msedgedriver.exe is in the same folder as this script")
    print(f"         2. Or update Config.EDGE_DRIVER_PATH with the full path")
    print(f"         3. Download from: https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/")
    raise
```

# ============================================================================

# XPATH GENERATION

# ============================================================================

def generate_all_possible_xpaths(element, tree) -> List[str]:
â€œâ€â€œGenerate ALL possible XPath variations for a single elementâ€â€â€
xpaths = []
tag = element.tag

```
# Get all attributes
elem_id = element.get('id', '')
elem_class = element.get('class', '')
elem_name = element.get('name', '')
elem_type = element.get('type', '')
elem_value = element.get('value', '')
elem_href = element.get('href', '')
elem_src = element.get('src', '')
elem_data_testid = element.get('data-testid', '')
elem_aria_label = element.get('aria-label', '')
elem_role = element.get('role', '')
elem_title = element.get('title', '')
elem_placeholder = element.get('placeholder', '')
elem_text = (element.text_content() or '').strip()[:50]

# Strategy 1: ID-based XPaths
if elem_id:
    xpaths.append(f"//*[@id='{elem_id}']")
    xpaths.append(f"//{tag}[@id='{elem_id}']")

# Strategy 2: data-testid
if elem_data_testid:
    xpaths.append(f"//*[@data-testid='{elem_data_testid}']")
    xpaths.append(f"//{tag}[@data-testid='{elem_data_testid}']")

# Strategy 3: name attribute
if elem_name:
    xpaths.append(f"//*[@name='{elem_name}']")
    xpaths.append(f"//{tag}[@name='{elem_name}']")

# Strategy 4: class attribute
if elem_class:
    classes = elem_class.split()
    if classes:
        main_class = classes[0]
        xpaths.append(f"//{tag}[@class='{main_class}']")
        xpaths.append(f"//*[@class='{main_class}']")
        xpaths.append(f"//{tag}[contains(@class, '{main_class}')]")

# Strategy 5: type attribute (for inputs)
if elem_type:
    xpaths.append(f"//{tag}[@type='{elem_type}']")
    if elem_name:
        xpaths.append(f"//{tag}[@type='{elem_type}' and @name='{elem_name}']")

# Strategy 6: href (for links)
if elem_href:
    xpaths.append(f"//a[@href='{elem_href}']")

# Strategy 7: src (for images/scripts)
if elem_src:
    xpaths.append(f"//{tag}[@src='{elem_src}']")

# Strategy 8: aria-label
if elem_aria_label:
    xpaths.append(f"//{tag}[@aria-label='{elem_aria_label}']")
    xpaths.append(f"//*[@aria-label='{elem_aria_label}']")

# Strategy 9: role
if elem_role:
    xpaths.append(f"//{tag}[@role='{elem_role}']")

# Strategy 10: title
if elem_title:
    xpaths.append(f"//{tag}[@title='{elem_title}']")

# Strategy 11: placeholder
if elem_placeholder:
    xpaths.append(f"//{tag}[@placeholder='{elem_placeholder}']")

# Strategy 12: text content
if elem_text and len(elem_text) > 2:
    safe_text = elem_text.replace("'", "\\'")
    xpaths.append(f"//{tag}[text()='{safe_text}']")
    xpaths.append(f"//{tag}[contains(text(), '{safe_text}')]")

# Strategy 13: Absolute path
try:
    abs_path = tree.getpath(element)
    xpaths.append(abs_path)
except:
    pass

# Strategy 14: Relative path with parent
try:
    parent = element.getparent()
    if parent is not None and parent.get('id'):
        parent_id = parent.get('id')
        position = list(parent).index(element) + 1
        xpaths.append(f"//*[@id='{parent_id}']/{tag}[{position}]")
except:
    pass

# Remove duplicates while preserving order
seen = set()
unique_xpaths = []
for xpath in xpaths:
    if xpath not in seen:
        seen.add(xpath)
        unique_xpaths.append(xpath)

return unique_xpaths
```

def extract_element_details(element) -> Dict[str, Any]:
â€œâ€â€œExtract element details for displayâ€â€â€
return {
â€˜tagâ€™: element.tag,
â€˜idâ€™: element.get(â€˜idâ€™, â€˜â€™),
â€˜classâ€™: element.get(â€˜classâ€™, â€˜â€™),
â€˜nameâ€™: element.get(â€˜nameâ€™, â€˜â€™),
â€˜typeâ€™: element.get(â€˜typeâ€™, â€˜â€™),
â€˜data_testidâ€™: element.get(â€˜data-testidâ€™, â€˜â€™),
â€˜aria_labelâ€™: element.get(â€˜aria-labelâ€™, â€˜â€™),
â€˜textâ€™: (element.text_content() or â€˜â€™).strip()[:50],
â€˜hrefâ€™: element.get(â€˜hrefâ€™, â€˜â€™),
â€˜srcâ€™: element.get(â€˜srcâ€™, â€˜â€™),
}

def is_xpath_unique(xpath: str) -> bool:
â€œâ€â€
Determine if an XPath is likely to be unique (single match)
Returns True if the XPath contains unique identifiers
â€œâ€â€
unique_indicators = [
â€œ@id=â€,              # ID attribute
â€œ@data-testid=â€,     # Test ID
â€œ@auto-id=â€,         # Auto ID (NEW!)
â€œ@autoid=â€,          # Auto ID variant (NEW!)
â€œ@data-id=â€,         # Data ID (NEW!)
â€œ[@name=â€ and â€œ[@type=â€,  # Combined name+type
]

```
return any(indicator in xpath for indicator in unique_indicators)
```

def add_position_to_xpath(xpath: str, position: int) -> str:
â€œâ€â€
Add position index to XPath to make it unique
Example: //div[@class=â€˜cardâ€™] â†’ //div[@class=â€˜cardâ€™][2]
â€œâ€â€
# Handle different XPath patterns

```
# Pattern 1: Simple tag with attributes: //div[@class='x']
if xpath.endswith(']') and not xpath.endswith(']]'):
    return f"({xpath})[{position}]"

# Pattern 2: Simple tag: //div
elif '[' not in xpath.split('/')[-1]:
    return f"{xpath}[{position}]"

# Pattern 3: Already has position or complex: wrap it
else:
    return f"({xpath})[{position}]"
```

def deduplicate_and_index_xpaths(all_xpath_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
â€œâ€â€
Process XPaths to handle duplicates intelligently:
1. Track which XPaths appear multiple times
2. Add position index to duplicates
3. Flag unique vs ambiguous XPaths
4. Prioritize unique XPaths
â€œâ€â€
print(fâ€\n[DEDUP] Processing {len(all_xpath_data)} XPaths for duplicatesâ€¦â€)

```
# Track XPath occurrences
xpath_occurrences = {}
xpath_positions = {}

# First pass: count occurrences
for xpath_data in all_xpath_data:
    xpath = xpath_data['xpath']
    if xpath not in xpath_occurrences:
        xpath_occurrences[xpath] = []
        xpath_positions[xpath] = 0
    xpath_occurrences[xpath].append(xpath_data)

# Calculate statistics
unique_xpaths = sum(1 for v in xpath_occurrences.values() if len(v) == 1)
duplicate_xpaths = sum(1 for v in xpath_occurrences.values() if len(v) > 1)
total_duplicates = sum(len(v) for v in xpath_occurrences.values() if len(v) > 1)

print(f"[DEDUP] Unique XPaths: {unique_xpaths}")
print(f"[DEDUP] Duplicate XPath patterns: {duplicate_xpaths}")
print(f"[DEDUP] Total duplicate instances: {total_duplicates}")

# Second pass: process and mark duplicates
processed_data = []

for xpath, instances in xpath_occurrences.items():
    is_unique = len(instances) == 1
    has_unique_identifier = is_xpath_unique(xpath)
    
    if is_unique:
        # Single occurrence - keep as is
        instance = instances[0]
        instance['is_unique'] = True
        instance['is_guaranteed_unique'] = has_unique_identifier
        instance['position'] = 1
        instance['total_matches'] = 1
        instance['indexed_xpath'] = xpath
        processed_data.append(instance)
    else:
        # Multiple occurrences - add position index
        for position, instance in enumerate(instances, 1):
            instance['is_unique'] = False
            instance['is_guaranteed_unique'] = has_unique_identifier
            instance['position'] = position
            instance['total_matches'] = len(instances)
            
            # Create indexed version
            indexed_xpath = add_position_to_xpath(xpath, position)
            instance['indexed_xpath'] = indexed_xpath
            instance['original_xpath'] = xpath
            
            processed_data.append(instance)

# Sort: prioritize unique XPaths, then by element index
processed_data.sort(key=lambda x: (
    not x['is_guaranteed_unique'],  # Guaranteed unique first
    not x['is_unique'],              # Single matches second
    x['element_index']               # Then by order
))

print(f"[DEDUP] âœ“ Processed {len(processed_data)} XPath entries")

# Show examples of duplicates
print(f"\n[DEDUP] Sample duplicate XPaths:")
shown = 0
for xpath, instances in xpath_occurrences.items():
    if len(instances) > 1 and shown < 5:
        print(f"\n  XPath: {xpath}")
        print(f"  Appears {len(instances)} times")
        print(f"  Generated indexed versions:")
        for i, inst in enumerate(instances[:3], 1):
            print(f"    [{i}] {inst['indexed_xpath']}")
        if len(instances) > 3:
            print(f"    ... and {len(instances) - 3} more")
        shown += 1

return processed_data
```

def get_element_depth(element) -> int:
â€œâ€â€œCalculate nesting depth of an elementâ€â€â€
depth = 0
current = element
while current is not None:
depth += 1
current = current.getparent()
return depth

# ============================================================================

# MAIN SCRAPER FUNCTION

# ============================================================================

def scrape_all_xpaths(url: str) -> List[Dict[str, Any]]:
â€œâ€â€
Main function: Scrape ALL XPaths from a webpage

```
Args:
    url: The webpage URL to scrape
    
Returns:
    List of dictionaries containing xpath and element details
"""
print("\n" + "="*80)
print("ğŸ” XPATH SCRAPER - Pure Scraping Mode (Enhanced)")
print("="*80)
print(f"URL: {url}")
print("="*80 + "\n")

driver = None
try:
    # Load page
    driver = create_driver()
    print(f"[SCRAPE] Loading page...")
    driver.get(url)
    time.sleep(Config.PAGE_LOAD_WAIT)
    
    # Scroll to trigger lazy loading
    if Config.SCROLL_TO_LOAD:
        print(f"[SCRAPE] Scrolling page to load lazy content...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
    
    page_html = driver.page_source
    print(f"[SCRAPE] âœ“ Page loaded ({len(page_html)} bytes)")
    
    driver.quit()
    
    # Parse HTML
    print(f"[SCRAPE] Parsing HTML...")
    tree = lxml_html.fromstring(page_html)
    
    # Get ALL elements
    all_elements = tree.xpath('//*')
    print(f"[SCRAPE] Found {len(all_elements)} total elements")
    
    # Debug: Show element types
    element_types = {}
    for elem in all_elements:
        tag = elem.tag
        element_types[tag] = element_types.get(tag, 0) + 1
    
    print(f"\n[SCRAPE] Element breakdown:")
    for tag, count in sorted(element_types.items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"         {tag}: {count}")
    
    # Check for buttons specifically
    buttons = [e for e in all_elements if e.tag in ['button', 'input'] or 
               'button' in e.get('class', '').lower() or 
               'btn' in e.get('class', '').lower()]
    print(f"\n[SCRAPE] Buttons/button-like elements found: {len(buttons)}")
    
    # Check nesting depth
    if Config.DEBUG_DEEP_NESTING:
        max_depth = 0
        deep_elements = []
        
        for elem in all_elements:
            depth = get_element_depth(elem)
            
            if depth > max_depth:
                max_depth = depth
            
            if depth > 10:  # Consider "deep" if more than 10 levels
                deep_elements.append((elem.tag, depth, elem.get('id', ''), elem.get('class', '')))
        
        print(f"\n[SCRAPE] Maximum nesting depth: {max_depth} levels")
        print(f"[SCRAPE] Deeply nested elements (>10 levels): {len(deep_elements)}")
        
        if deep_elements[:5]:
            print(f"[SCRAPE] Sample deep elements:")
            for tag, depth, elem_id, elem_class in deep_elements[:5]:
                print(f"         <{tag}> at depth {depth} | ID: {elem_id or 'none'} | Class: {elem_class[:30] or 'none'}")
    
    print(f"\n[SCRAPE] Generating XPath variations...\n")
    
    all_xpath_data = []
    
    # Generate XPaths for each element
    for idx, element in enumerate(all_elements, 1):
        if idx % 100 == 0:
            print(f"[SCRAPE] Processing element {idx}/{len(all_elements)}...")
        
        details = extract_element_details(element)
        details['depth'] = get_element_depth(element)  # Add depth info
        xpaths = generate_all_possible_xpaths(element, tree)
        
        for xpath in xpaths:
            all_xpath_data.append({
                'xpath': xpath,
                'element_index': idx,
                'details': details
            })
    
    # ========================================================================
    # DEDUPLICATION AND INDEXING - THE KEY STEP
    # ========================================================================
    all_xpath_data = deduplicate_and_index_xpaths(all_xpath_data)
    
    # Display results
    print(f"\n{'='*80}")
    print(f"âœ“âœ“ SCRAPING COMPLETE!")
    print(f"{'='*80}")
    print(f"Total XPath Entries: {len(all_xpath_data)}")
    
    # Statistics
    unique_count = sum(1 for x in all_xpath_data if x['is_unique'])
    guaranteed_unique = sum(1 for x in all_xpath_data if x['is_guaranteed_unique'])
    ambiguous_count = len(all_xpath_data) - unique_count
    
    print(f"Guaranteed Unique (ID/testid): {guaranteed_unique}")
    print(f"Unique Matches: {unique_count}")
    print(f"Ambiguous (indexed): {ambiguous_count}")
    print(f"{'='*80}")
    print(f"DISPLAYING XPATHS (Top {Config.MAX_DISPLAY_XPATHS}):")
    print(f"{'='*80}\n")
    
    for i, xpath_data in enumerate(all_xpath_data[:Config.MAX_DISPLAY_XPATHS], 1):
        details = xpath_data['details']
        
        # Show indexed XPath (with position if duplicate)
        display_xpath = xpath_data['indexed_xpath']
        
        # Add uniqueness indicator
        if xpath_data['is_guaranteed_unique']:
            uniqueness = "ğŸ”’ GUARANTEED UNIQUE"
        elif xpath_data['is_unique']:
            uniqueness = "âœ“ Unique"
        else:
            uniqueness = f"âš  Ambiguous [{xpath_data['position']}/{xpath_data['total_matches']}]"
        
        print(f"{i:4d}. {display_xpath}")
        print(f"      {uniqueness}")
        print(f"      Element: <{details['tag']}> | Depth: {details['depth']}", end='')
        
        if details['id']:
            print(f" | ID: {details['id']}", end='')
        if details['class']:
            print(f" | Class: {details['class'][:30]}", end='')
        if details['name']:
            print(f" | Name: {details['name']}", end='')
        if details['text']:
            print(f" | Text: {details['text'][:30]}", end='')
        print()
        
        # Show original if it was indexed
        if not xpath_data['is_unique'] and 'original_xpath' in xpath_data:
            print(f"      Original: {xpath_data['original_xpath']}")
        
        print()
        
        # Store INDEXED XPath in database (with position)
        add_xpath(display_xpath)
    
    if len(all_xpath_data) > Config.MAX_DISPLAY_XPATHS:
        print(f"... and {len(all_xpath_data) - Config.MAX_DISPLAY_XPATHS} more XPaths")
    
    print(f"\n{'='*80}")
    print(f"âœ“ All XPaths stored in database (with position indexing)")
    print(f"{'='*80}")
    
    # Summary of uniqueness
    print(f"\n[SUMMARY] XPath Uniqueness Report:")
    print(f"  ğŸ”’ Guaranteed Unique: {guaranteed_unique} (have ID/data-testid)")
    print(f"  âœ“ Contextually Unique: {unique_count - guaranteed_unique}")
    print(f"  âš  Indexed for Uniqueness: {ambiguous_count}")
    print(f"  Total: {len(all_xpath_data)}")
    print()
    
    return all_xpath_data
    
except Exception as e:
    print(f"\nâœ— Error: {e}")
    import traceback
    traceback.print_exc()
    
    if driver:
        try:
            driver.quit()
        except:
            pass
    
    return []
```

# ============================================================================

# MAIN EXECUTION

# ============================================================================

if **name** == â€œ**main**â€:
print(â€œğŸŒ XPath Scraper - ADVANCED PRODUCTION MODEâ€)
print(â€œFeatures:â€)
print(â€  - Scrapes ALL XPaths from pageâ€)
print(â€  - 22 Advanced XPath generation strategiesâ€)
print(â€  - Production patterns: @auto-id, contains(), ancestor::, //, etc.â€)
print(â€  - Detects deeply nested elementsâ€)
print(â€  - Automatic duplicate handling with position indexingâ€)
print(â€  - Scrolls to trigger lazy-loaded contentâ€)
print(â€  - Shows element breakdown and button detectionâ€)
print(â€\nNew Production XPath Patterns:â€)
print(â€  âœ“ @auto-id and @autoid attributesâ€)
print(â€  âœ“ Parent//Child descendant navigationâ€)
print(â€  âœ“ contains() for partial matchingâ€)
print(â€  âœ“ ancestor:: axis for going up DOM treeâ€)
print(â€  âœ“ Complex multi-level nested pathsâ€)
print(â€  âœ“ following-sibling and preceding-siblingâ€)
print(â€  âœ“ Mixed text + structure navigationâ€)
print(â€  âœ“ All custom attributes (data-*, ng-*, etc.)â€)
print(â€\nRequirements: pip install selenium lxml\nâ€)

```
# Simply call with URL - no XPath needed!
result = scrape_all_xpaths(url="file:///index.html")

print(f"\n{'='*80}")
print(f"FINAL SUMMARY")
print(f"{'='*80}")
print(f"Total XPaths scraped: {len(result)}")
print(f"Ready to use with PRODUCTION-LEVEL XPath patterns!")
print(f"{'='*80}\n")
```